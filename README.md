3.
Attention mechanism is an important technique for processing sequence data. It allows the model to dynamically allocate different degrees of attention to different parts of the input sequence while processing the input sequence, thus improving the modeling capability of the model for long-distance dependencies.
Multi-attention is an extended form of attention mechanism. It calculates multiple attention heads in parallel, each head learns and focuses on different feature subspaces, and then combines the results of these heads to enhance the modeling ability of the relationship between different features.

4.
1. Increased communication overhead: In data parallelism, different workers need to exchange gradient information in each iteration to update parameters. In layer parallelism, each worker node is responsible for computing part of the output of the neural network layer. When the two are combined, more frequent communication between each node is required to ensure that each node has the required gradient information and layer output, increasing communication overhead.
2. Synchronization problem: Both data parallelism and layer parallelism involve synchronization between different workers. In data parallelism, synchronization is usually achieved by waiting for all nodes to complete the calculation, while in layer parallelism, synchronization ensures that the output of each layer is ready to go to the next layer. When the two are combined, it is necessary to solve how to effectively synchronize the calculation and parameter update between different workers to ensure the consistency and correctness of the whole network.
3. Load balancing: In data parallelism and layer parallelism, different worker nodes may process different data fragments or neural network layers, resulting in load imbalance. When combining the two, you need to consider how to effectively distribute the workload to ensure that each node is fully utilizing its computing resources and to minimize computing differences between nodes.
4. Decreased resource utilization: Both data parallelism and tier parallelism involve multiple worker nodes performing computing at the same time. Therefore, the utilization of computing resources may decrease. When combining the two, ensure that each node can effectively use computing resources to avoid resource waste.
