3.
Attention mechanism is an important technique for processing sequence data. It allows the model to dynamically allocate different degrees of attention to different parts of the input sequence while processing the input sequence, thus improving the modeling capability of the model for long-distance dependencies.
Multi-attention is an extended form of attention mechanism. It calculates multiple attention heads in parallel, each head learns and focuses on different feature subspaces, and then combines the results of these heads to enhance the modeling ability of the relationship between different features.

